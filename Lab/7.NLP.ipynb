{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7.NLP.ipynb","provenance":[],"authorship_tag":"ABX9TyMhuvX6s+sET4XCEpICU+hw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"DEwBVogbVAVe","colab_type":"code","outputId":"ac5edf61-f1da-4847-dcc9-0fbc697871ee","executionInfo":{"status":"ok","timestamp":1583895955407,"user_tz":300,"elapsed":9099,"user":{"displayName":"divya reddy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggs2myGpo2b2t9Kz29mUuWgLcVrvayYSRaaD3Op_TQ=s64","userId":"04397886005787697308"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["f = open('nlp_input.txt', 'rb')\n","\n","input = f.read()\n","\n","\n","\n","# Task(b): Tokenize the text into words\n","\n","import nltk\n","\n","nltk.download('punkt')\n","\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","input = input.decode('latin-1') \n","\n","print(sent_tokenize(input))\n","\n","wordTokens = word_tokenize(input)\n","\n","print(wordTokens) \n","\n","# Task(b): Apply lemmatization technique on each word.\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","\n","for wordToken in wordTokens:\n","\n","    print(lemmatizer.lemmatize(wordToken))\n","\n","print(\"\\n\")\n","\n","# Task(c): Trigram\n","\n","triDic = {}\n","\n","count =1\n","\n","trigrams = nltk.trigrams(input.split())\n","\n","print(\"----Task(c): Trigram\\n\")\n","\n","for trigram in trigrams:\n","\n","    dicKey=' '.join(trigram)\n","\n","    if dicKey in triDic.keys():\n","\n","        triDic.update({dicKey:triDic[dicKey]+1})\n","\n","    else:\n","\n","        triDic[dicKey]=1\n","\n","print(triDic,\"\\n\")\n","\n","#Task(d) to get top 10 values from dictionary\n","\n","from heapq import nlargest\n","\n","\n","\n","TenHighest = nlargest(10, triDic, key=triDic.get)\n","\n","\n","\n","print(\"----Task(d):Dictionary with 10 highest values:\")\n","\n","for val in TenHighest:\n","\n","    print(val, \":\", triDic.get(val))\n","\n","print(\"\\n\")\n","\n","#Task(e & f ): Tokenizing into sentence and Find all the sentences with the most repeated tri-grams\n","\n","# Task (g & h): Extract those sentences and concatenate and print the concatenated result\n","\n","\n","\n","sentTokens = nltk.sent_tokenize(input)\n","\n","maxValue = max(triDic, key=triDic.get)\n","\n","concatenatedString=''\n","\n","print(\"Task(e,f) Combined\")\n","\n","print(\"----All the sentences with the most repeated tri-grams----\\n\")\n","\n","for stoken in sentTokens:\n","\n","    if maxValue in stoken:\n","\n","       print(stoken)\n","\n","       concatenatedString = concatenatedString+stoken\n","\n","print(\"\\n\")\n","\n","print(\"Task(g,h) Combined\")\n","\n","print(\"----Concatenated Result----\\n\")\n","\n","print(concatenatedString)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['Regression analysis is a statistical technique that models and approximates the relationship between a dependent and one or more independent variables.', 'This article will quickly introduce three commonly used regression models using R and the Boston housing data-set: Ridge, Lasso, and Elastic Net.', 'First we need to understand the basics of regression and what parameters of the equation are changed when using a specific model.', 'Simple linear regression, also known as ordinary least squares (OLS) attempts to minimize the sum of error squared.', 'The error in this case is the difference between the actual data point and its predicted value.', 'Visualization of the squared error (from Setosa.io)\\r\\nThe equation for this model is referred to as the cost function and is a way to find the optimal error by minimizing and measuring it.', 'The gradient descent algorithm is used to find the optimal cost function by going over a number of iterations.', 'But the data we need to define and analyze is not always so easy to characterize with the base OLS model.', 'Equation for least ordinary squares\\r\\nOne situation is the data showing multi-collinearity, this is when predictor variables are correlated to each other and to the response variable.', 'To picture this let\\x92s say we\\x92re doing a study that looks at a response variable?\\x97?patient weight, and our predictor variables would be height, sex, and diet.', 'The problem here is that height and sex are also correlated and can inflate the standard error of their coefficients which may make them seem statistically insignificant.', 'To produce a more accurate model of complex data we can add a penalty term to the OLS equation.', 'A penalty adds a bias towards certain values.', 'These are known as L1 regularization(Lasso regression) and L2 regularization(ridge regression).The best model we can hope to come up with minimizes both the bias and the variance:\\r\\nRidge regression uses L2 regularization which adds the following penalty term to the OLS equation.', 'L2 regularization penalty term\\r\\nThe L2 term is equal to the square of the magnitude of the coefficients.', 'In this case if lambda(?)', 'is zero then the equation is the basic OLS but if it is greater than zero then we add a constraint to the coefficients.', 'This constraint results in minimized coefficients (aka shrinkage) that trend towards zero the larger the value of lambda.', 'Shrinking the coefficients leads to a lower variance and in turn a lower error value.', 'Therefore Ridge regression decreases the complexity of a model but does not reduce the number of variables, it rather just shrinks their effect.', 'Lasso regression\\r\\nLasso regression uses the L1 penalty term and stands for Least Absolute Shrinkage and Selection Operator.', 'The penalty applied for L2 is equal to the absolute value of the magnitude of the coefficients:\\r\\nL1 regularization penalty term\\r\\nSimilar to ridge regression, a lambda value of zero spits out the basic OLS equation, however given a suitable lambda value lasso regression can drive some coefficients to zero.', 'The larger the value of lambda the more features are shrunk to zero.', 'This can eliminate some features entirely and give us a subset of predictors that helps mitigate multi-collinearity and model complexity.', 'Predictors not shrunk towards zero signify that they are important and thus L1 regularization allows for feature selection (sparse selection).', 'A third commonly used model of regression is the Elastic Net which incorporates penalties from both L1 and L2 regularization:\\r\\nIn addition to setting and choosing a lambda value elastic net also allows us to tune the alpha parameter where ??', '= 0 corresponds to ridge and ??', '= 1 to lasso.', 'Simply put, if you plug in 0 for alpha, the penalty function reduces to the L1 (ridge) term and if we set alpha to 1 we get the L2 (lasso) term.', 'Therefore we can choose an alpha value between 0 and 1 to optimize the elastic net.', 'Effectively this will shrink some coefficients and set some to 0 for sparse selection.', 'As we mentioned in the previous sections, lambda values have a large effect on coefficients so now we will compute and chose a suitable one.', 'Here we perform a cross validation and take a peek at the lambda value corresponding to the lowest prediction error before fitting the data to the model and viewing the coefficients.', 'We can see here that certain coefficients have been pushed towards zero and minimized while RM (number of rooms) has a significantly higher weight than the rest\\r\\nPerforming Lasso regression\\r\\nThe steps will be identical to what we have done for ridge regression.', 'The value of alpha is the only change here (remember ??', '= 1 denotes lasso)\\r\\nPerforming Elastic Net regression\\r\\nPerforming Elastic Net requires us to tune parameters to identify the best alpha and lambda values and for this we need to use the caret package.', 'We will tune the model by iterating over a number of alpha and lambda pairs and we can see which pair has the lowest associated error.', 'We can see that the R mean-squared values using all three models were very close to each other, but both did marginally perform better than ridge regression (Lasso having done best).', 'Lasso regression also showed the highest RÂ² value.']\n","['Regression', 'analysis', 'is', 'a', 'statistical', 'technique', 'that', 'models', 'and', 'approximates', 'the', 'relationship', 'between', 'a', 'dependent', 'and', 'one', 'or', 'more', 'independent', 'variables', '.', 'This', 'article', 'will', 'quickly', 'introduce', 'three', 'commonly', 'used', 'regression', 'models', 'using', 'R', 'and', 'the', 'Boston', 'housing', 'data-set', ':', 'Ridge', ',', 'Lasso', ',', 'and', 'Elastic', 'Net', '.', 'First', 'we', 'need', 'to', 'understand', 'the', 'basics', 'of', 'regression', 'and', 'what', 'parameters', 'of', 'the', 'equation', 'are', 'changed', 'when', 'using', 'a', 'specific', 'model', '.', 'Simple', 'linear', 'regression', ',', 'also', 'known', 'as', 'ordinary', 'least', 'squares', '(', 'OLS', ')', 'attempts', 'to', 'minimize', 'the', 'sum', 'of', 'error', 'squared', '.', 'The', 'error', 'in', 'this', 'case', 'is', 'the', 'difference', 'between', 'the', 'actual', 'data', 'point', 'and', 'its', 'predicted', 'value', '.', 'Visualization', 'of', 'the', 'squared', 'error', '(', 'from', 'Setosa.io', ')', 'The', 'equation', 'for', 'this', 'model', 'is', 'referred', 'to', 'as', 'the', 'cost', 'function', 'and', 'is', 'a', 'way', 'to', 'find', 'the', 'optimal', 'error', 'by', 'minimizing', 'and', 'measuring', 'it', '.', 'The', 'gradient', 'descent', 'algorithm', 'is', 'used', 'to', 'find', 'the', 'optimal', 'cost', 'function', 'by', 'going', 'over', 'a', 'number', 'of', 'iterations', '.', 'But', 'the', 'data', 'we', 'need', 'to', 'define', 'and', 'analyze', 'is', 'not', 'always', 'so', 'easy', 'to', 'characterize', 'with', 'the', 'base', 'OLS', 'model', '.', 'Equation', 'for', 'least', 'ordinary', 'squares', 'One', 'situation', 'is', 'the', 'data', 'showing', 'multi-collinearity', ',', 'this', 'is', 'when', 'predictor', 'variables', 'are', 'correlated', 'to', 'each', 'other', 'and', 'to', 'the', 'response', 'variable', '.', 'To', 'picture', 'this', 'let\\x92s', 'say', 'we\\x92re', 'doing', 'a', 'study', 'that', 'looks', 'at', 'a', 'response', 'variable', '?', '\\x97', '?', 'patient', 'weight', ',', 'and', 'our', 'predictor', 'variables', 'would', 'be', 'height', ',', 'sex', ',', 'and', 'diet', '.', 'The', 'problem', 'here', 'is', 'that', 'height', 'and', 'sex', 'are', 'also', 'correlated', 'and', 'can', 'inflate', 'the', 'standard', 'error', 'of', 'their', 'coefficients', 'which', 'may', 'make', 'them', 'seem', 'statistically', 'insignificant', '.', 'To', 'produce', 'a', 'more', 'accurate', 'model', 'of', 'complex', 'data', 'we', 'can', 'add', 'a', 'penalty', 'term', 'to', 'the', 'OLS', 'equation', '.', 'A', 'penalty', 'adds', 'a', 'bias', 'towards', 'certain', 'values', '.', 'These', 'are', 'known', 'as', 'L1', 'regularization', '(', 'Lasso', 'regression', ')', 'and', 'L2', 'regularization', '(', 'ridge', 'regression', ')', '.The', 'best', 'model', 'we', 'can', 'hope', 'to', 'come', 'up', 'with', 'minimizes', 'both', 'the', 'bias', 'and', 'the', 'variance', ':', 'Ridge', 'regression', 'uses', 'L2', 'regularization', 'which', 'adds', 'the', 'following', 'penalty', 'term', 'to', 'the', 'OLS', 'equation', '.', 'L2', 'regularization', 'penalty', 'term', 'The', 'L2', 'term', 'is', 'equal', 'to', 'the', 'square', 'of', 'the', 'magnitude', 'of', 'the', 'coefficients', '.', 'In', 'this', 'case', 'if', 'lambda', '(', '?', ')', 'is', 'zero', 'then', 'the', 'equation', 'is', 'the', 'basic', 'OLS', 'but', 'if', 'it', 'is', 'greater', 'than', 'zero', 'then', 'we', 'add', 'a', 'constraint', 'to', 'the', 'coefficients', '.', 'This', 'constraint', 'results', 'in', 'minimized', 'coefficients', '(', 'aka', 'shrinkage', ')', 'that', 'trend', 'towards', 'zero', 'the', 'larger', 'the', 'value', 'of', 'lambda', '.', 'Shrinking', 'the', 'coefficients', 'leads', 'to', 'a', 'lower', 'variance', 'and', 'in', 'turn', 'a', 'lower', 'error', 'value', '.', 'Therefore', 'Ridge', 'regression', 'decreases', 'the', 'complexity', 'of', 'a', 'model', 'but', 'does', 'not', 'reduce', 'the', 'number', 'of', 'variables', ',', 'it', 'rather', 'just', 'shrinks', 'their', 'effect', '.', 'Lasso', 'regression', 'Lasso', 'regression', 'uses', 'the', 'L1', 'penalty', 'term', 'and', 'stands', 'for', 'Least', 'Absolute', 'Shrinkage', 'and', 'Selection', 'Operator', '.', 'The', 'penalty', 'applied', 'for', 'L2', 'is', 'equal', 'to', 'the', 'absolute', 'value', 'of', 'the', 'magnitude', 'of', 'the', 'coefficients', ':', 'L1', 'regularization', 'penalty', 'term', 'Similar', 'to', 'ridge', 'regression', ',', 'a', 'lambda', 'value', 'of', 'zero', 'spits', 'out', 'the', 'basic', 'OLS', 'equation', ',', 'however', 'given', 'a', 'suitable', 'lambda', 'value', 'lasso', 'regression', 'can', 'drive', 'some', 'coefficients', 'to', 'zero', '.', 'The', 'larger', 'the', 'value', 'of', 'lambda', 'the', 'more', 'features', 'are', 'shrunk', 'to', 'zero', '.', 'This', 'can', 'eliminate', 'some', 'features', 'entirely', 'and', 'give', 'us', 'a', 'subset', 'of', 'predictors', 'that', 'helps', 'mitigate', 'multi-collinearity', 'and', 'model', 'complexity', '.', 'Predictors', 'not', 'shrunk', 'towards', 'zero', 'signify', 'that', 'they', 'are', 'important', 'and', 'thus', 'L1', 'regularization', 'allows', 'for', 'feature', 'selection', '(', 'sparse', 'selection', ')', '.', 'A', 'third', 'commonly', 'used', 'model', 'of', 'regression', 'is', 'the', 'Elastic', 'Net', 'which', 'incorporates', 'penalties', 'from', 'both', 'L1', 'and', 'L2', 'regularization', ':', 'In', 'addition', 'to', 'setting', 'and', 'choosing', 'a', 'lambda', 'value', 'elastic', 'net', 'also', 'allows', 'us', 'to', 'tune', 'the', 'alpha', 'parameter', 'where', '?', '?', '=', '0', 'corresponds', 'to', 'ridge', 'and', '?', '?', '=', '1', 'to', 'lasso', '.', 'Simply', 'put', ',', 'if', 'you', 'plug', 'in', '0', 'for', 'alpha', ',', 'the', 'penalty', 'function', 'reduces', 'to', 'the', 'L1', '(', 'ridge', ')', 'term', 'and', 'if', 'we', 'set', 'alpha', 'to', '1', 'we', 'get', 'the', 'L2', '(', 'lasso', ')', 'term', '.', 'Therefore', 'we', 'can', 'choose', 'an', 'alpha', 'value', 'between', '0', 'and', '1', 'to', 'optimize', 'the', 'elastic', 'net', '.', 'Effectively', 'this', 'will', 'shrink', 'some', 'coefficients', 'and', 'set', 'some', 'to', '0', 'for', 'sparse', 'selection', '.', 'As', 'we', 'mentioned', 'in', 'the', 'previous', 'sections', ',', 'lambda', 'values', 'have', 'a', 'large', 'effect', 'on', 'coefficients', 'so', 'now', 'we', 'will', 'compute', 'and', 'chose', 'a', 'suitable', 'one', '.', 'Here', 'we', 'perform', 'a', 'cross', 'validation', 'and', 'take', 'a', 'peek', 'at', 'the', 'lambda', 'value', 'corresponding', 'to', 'the', 'lowest', 'prediction', 'error', 'before', 'fitting', 'the', 'data', 'to', 'the', 'model', 'and', 'viewing', 'the', 'coefficients', '.', 'We', 'can', 'see', 'here', 'that', 'certain', 'coefficients', 'have', 'been', 'pushed', 'towards', 'zero', 'and', 'minimized', 'while', 'RM', '(', 'number', 'of', 'rooms', ')', 'has', 'a', 'significantly', 'higher', 'weight', 'than', 'the', 'rest', 'Performing', 'Lasso', 'regression', 'The', 'steps', 'will', 'be', 'identical', 'to', 'what', 'we', 'have', 'done', 'for', 'ridge', 'regression', '.', 'The', 'value', 'of', 'alpha', 'is', 'the', 'only', 'change', 'here', '(', 'remember', '?', '?', '=', '1', 'denotes', 'lasso', ')', 'Performing', 'Elastic', 'Net', 'regression', 'Performing', 'Elastic', 'Net', 'requires', 'us', 'to', 'tune', 'parameters', 'to', 'identify', 'the', 'best', 'alpha', 'and', 'lambda', 'values', 'and', 'for', 'this', 'we', 'need', 'to', 'use', 'the', 'caret', 'package', '.', 'We', 'will', 'tune', 'the', 'model', 'by', 'iterating', 'over', 'a', 'number', 'of', 'alpha', 'and', 'lambda', 'pairs', 'and', 'we', 'can', 'see', 'which', 'pair', 'has', 'the', 'lowest', 'associated', 'error', '.', 'We', 'can', 'see', 'that', 'the', 'R', 'mean-squared', 'values', 'using', 'all', 'three', 'models', 'were', 'very', 'close', 'to', 'each', 'other', ',', 'but', 'both', 'did', 'marginally', 'perform', 'better', 'than', 'ridge', 'regression', '(', 'Lasso', 'having', 'done', 'best', ')', '.', 'Lasso', 'regression', 'also', 'showed', 'the', 'highest', 'RÂ²', 'value', '.']\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Regression\n","analysis\n","is\n","a\n","statistical\n","technique\n","that\n","model\n","and\n","approximates\n","the\n","relationship\n","between\n","a\n","dependent\n","and\n","one\n","or\n","more\n","independent\n","variable\n",".\n","This\n","article\n","will\n","quickly\n","introduce\n","three\n","commonly\n","used\n","regression\n","model\n","using\n","R\n","and\n","the\n","Boston\n","housing\n","data-set\n",":\n","Ridge\n",",\n","Lasso\n",",\n","and\n","Elastic\n","Net\n",".\n","First\n","we\n","need\n","to\n","understand\n","the\n","basic\n","of\n","regression\n","and\n","what\n","parameter\n","of\n","the\n","equation\n","are\n","changed\n","when\n","using\n","a\n","specific\n","model\n",".\n","Simple\n","linear\n","regression\n",",\n","also\n","known\n","a\n","ordinary\n","least\n","square\n","(\n","OLS\n",")\n","attempt\n","to\n","minimize\n","the\n","sum\n","of\n","error\n","squared\n",".\n","The\n","error\n","in\n","this\n","case\n","is\n","the\n","difference\n","between\n","the\n","actual\n","data\n","point\n","and\n","it\n","predicted\n","value\n",".\n","Visualization\n","of\n","the\n","squared\n","error\n","(\n","from\n","Setosa.io\n",")\n","The\n","equation\n","for\n","this\n","model\n","is\n","referred\n","to\n","a\n","the\n","cost\n","function\n","and\n","is\n","a\n","way\n","to\n","find\n","the\n","optimal\n","error\n","by\n","minimizing\n","and\n","measuring\n","it\n",".\n","The\n","gradient\n","descent\n","algorithm\n","is\n","used\n","to\n","find\n","the\n","optimal\n","cost\n","function\n","by\n","going\n","over\n","a\n","number\n","of\n","iteration\n",".\n","But\n","the\n","data\n","we\n","need\n","to\n","define\n","and\n","analyze\n","is\n","not\n","always\n","so\n","easy\n","to\n","characterize\n","with\n","the\n","base\n","OLS\n","model\n",".\n","Equation\n","for\n","least\n","ordinary\n","square\n","One\n","situation\n","is\n","the\n","data\n","showing\n","multi-collinearity\n",",\n","this\n","is\n","when\n","predictor\n","variable\n","are\n","correlated\n","to\n","each\n","other\n","and\n","to\n","the\n","response\n","variable\n",".\n","To\n","picture\n","this\n","letÂs\n","say\n","weÂre\n","doing\n","a\n","study\n","that\n","look\n","at\n","a\n","response\n","variable\n","?\n","Â\n","?\n","patient\n","weight\n",",\n","and\n","our\n","predictor\n","variable\n","would\n","be\n","height\n",",\n","sex\n",",\n","and\n","diet\n",".\n","The\n","problem\n","here\n","is\n","that\n","height\n","and\n","sex\n","are\n","also\n","correlated\n","and\n","can\n","inflate\n","the\n","standard\n","error\n","of\n","their\n","coefficient\n","which\n","may\n","make\n","them\n","seem\n","statistically\n","insignificant\n",".\n","To\n","produce\n","a\n","more\n","accurate\n","model\n","of\n","complex\n","data\n","we\n","can\n","add\n","a\n","penalty\n","term\n","to\n","the\n","OLS\n","equation\n",".\n","A\n","penalty\n","add\n","a\n","bias\n","towards\n","certain\n","value\n",".\n","These\n","are\n","known\n","a\n","L1\n","regularization\n","(\n","Lasso\n","regression\n",")\n","and\n","L2\n","regularization\n","(\n","ridge\n","regression\n",")\n",".The\n","best\n","model\n","we\n","can\n","hope\n","to\n","come\n","up\n","with\n","minimizes\n","both\n","the\n","bias\n","and\n","the\n","variance\n",":\n","Ridge\n","regression\n","us\n","L2\n","regularization\n","which\n","add\n","the\n","following\n","penalty\n","term\n","to\n","the\n","OLS\n","equation\n",".\n","L2\n","regularization\n","penalty\n","term\n","The\n","L2\n","term\n","is\n","equal\n","to\n","the\n","square\n","of\n","the\n","magnitude\n","of\n","the\n","coefficient\n",".\n","In\n","this\n","case\n","if\n","lambda\n","(\n","?\n",")\n","is\n","zero\n","then\n","the\n","equation\n","is\n","the\n","basic\n","OLS\n","but\n","if\n","it\n","is\n","greater\n","than\n","zero\n","then\n","we\n","add\n","a\n","constraint\n","to\n","the\n","coefficient\n",".\n","This\n","constraint\n","result\n","in\n","minimized\n","coefficient\n","(\n","aka\n","shrinkage\n",")\n","that\n","trend\n","towards\n","zero\n","the\n","larger\n","the\n","value\n","of\n","lambda\n",".\n","Shrinking\n","the\n","coefficient\n","lead\n","to\n","a\n","lower\n","variance\n","and\n","in\n","turn\n","a\n","lower\n","error\n","value\n",".\n","Therefore\n","Ridge\n","regression\n","decrease\n","the\n","complexity\n","of\n","a\n","model\n","but\n","doe\n","not\n","reduce\n","the\n","number\n","of\n","variable\n",",\n","it\n","rather\n","just\n","shrink\n","their\n","effect\n",".\n","Lasso\n","regression\n","Lasso\n","regression\n","us\n","the\n","L1\n","penalty\n","term\n","and\n","stand\n","for\n","Least\n","Absolute\n","Shrinkage\n","and\n","Selection\n","Operator\n",".\n","The\n","penalty\n","applied\n","for\n","L2\n","is\n","equal\n","to\n","the\n","absolute\n","value\n","of\n","the\n","magnitude\n","of\n","the\n","coefficient\n",":\n","L1\n","regularization\n","penalty\n","term\n","Similar\n","to\n","ridge\n","regression\n",",\n","a\n","lambda\n","value\n","of\n","zero\n","spit\n","out\n","the\n","basic\n","OLS\n","equation\n",",\n","however\n","given\n","a\n","suitable\n","lambda\n","value\n","lasso\n","regression\n","can\n","drive\n","some\n","coefficient\n","to\n","zero\n",".\n","The\n","larger\n","the\n","value\n","of\n","lambda\n","the\n","more\n","feature\n","are\n","shrunk\n","to\n","zero\n",".\n","This\n","can\n","eliminate\n","some\n","feature\n","entirely\n","and\n","give\n","u\n","a\n","subset\n","of\n","predictor\n","that\n","help\n","mitigate\n","multi-collinearity\n","and\n","model\n","complexity\n",".\n","Predictors\n","not\n","shrunk\n","towards\n","zero\n","signify\n","that\n","they\n","are\n","important\n","and\n","thus\n","L1\n","regularization\n","allows\n","for\n","feature\n","selection\n","(\n","sparse\n","selection\n",")\n",".\n","A\n","third\n","commonly\n","used\n","model\n","of\n","regression\n","is\n","the\n","Elastic\n","Net\n","which\n","incorporates\n","penalty\n","from\n","both\n","L1\n","and\n","L2\n","regularization\n",":\n","In\n","addition\n","to\n","setting\n","and\n","choosing\n","a\n","lambda\n","value\n","elastic\n","net\n","also\n","allows\n","u\n","to\n","tune\n","the\n","alpha\n","parameter\n","where\n","?\n","?\n","=\n","0\n","corresponds\n","to\n","ridge\n","and\n","?\n","?\n","=\n","1\n","to\n","lasso\n",".\n","Simply\n","put\n",",\n","if\n","you\n","plug\n","in\n","0\n","for\n","alpha\n",",\n","the\n","penalty\n","function\n","reduces\n","to\n","the\n","L1\n","(\n","ridge\n",")\n","term\n","and\n","if\n","we\n","set\n","alpha\n","to\n","1\n","we\n","get\n","the\n","L2\n","(\n","lasso\n",")\n","term\n",".\n","Therefore\n","we\n","can\n","choose\n","an\n","alpha\n","value\n","between\n","0\n","and\n","1\n","to\n","optimize\n","the\n","elastic\n","net\n",".\n","Effectively\n","this\n","will\n","shrink\n","some\n","coefficient\n","and\n","set\n","some\n","to\n","0\n","for\n","sparse\n","selection\n",".\n","As\n","we\n","mentioned\n","in\n","the\n","previous\n","section\n",",\n","lambda\n","value\n","have\n","a\n","large\n","effect\n","on\n","coefficient\n","so\n","now\n","we\n","will\n","compute\n","and\n","chose\n","a\n","suitable\n","one\n",".\n","Here\n","we\n","perform\n","a\n","cross\n","validation\n","and\n","take\n","a\n","peek\n","at\n","the\n","lambda\n","value\n","corresponding\n","to\n","the\n","lowest\n","prediction\n","error\n","before\n","fitting\n","the\n","data\n","to\n","the\n","model\n","and\n","viewing\n","the\n","coefficient\n",".\n","We\n","can\n","see\n","here\n","that\n","certain\n","coefficient\n","have\n","been\n","pushed\n","towards\n","zero\n","and\n","minimized\n","while\n","RM\n","(\n","number\n","of\n","room\n",")\n","ha\n","a\n","significantly\n","higher\n","weight\n","than\n","the\n","rest\n","Performing\n","Lasso\n","regression\n","The\n","step\n","will\n","be\n","identical\n","to\n","what\n","we\n","have\n","done\n","for\n","ridge\n","regression\n",".\n","The\n","value\n","of\n","alpha\n","is\n","the\n","only\n","change\n","here\n","(\n","remember\n","?\n","?\n","=\n","1\n","denotes\n","lasso\n",")\n","Performing\n","Elastic\n","Net\n","regression\n","Performing\n","Elastic\n","Net\n","requires\n","u\n","to\n","tune\n","parameter\n","to\n","identify\n","the\n","best\n","alpha\n","and\n","lambda\n","value\n","and\n","for\n","this\n","we\n","need\n","to\n","use\n","the\n","caret\n","package\n",".\n","We\n","will\n","tune\n","the\n","model\n","by\n","iterating\n","over\n","a\n","number\n","of\n","alpha\n","and\n","lambda\n","pair\n","and\n","we\n","can\n","see\n","which\n","pair\n","ha\n","the\n","lowest\n","associated\n","error\n",".\n","We\n","can\n","see\n","that\n","the\n","R\n","mean-squared\n","value\n","using\n","all\n","three\n","model\n","were\n","very\n","close\n","to\n","each\n","other\n",",\n","but\n","both\n","did\n","marginally\n","perform\n","better\n","than\n","ridge\n","regression\n","(\n","Lasso\n","having\n","done\n","best\n",")\n",".\n","Lasso\n","regression\n","also\n","showed\n","the\n","highest\n","RÂ²\n","value\n",".\n","\n","\n","----Task(c): Trigram\n","\n","{'Regression analysis is': 1, 'analysis is a': 1, 'is a statistical': 1, 'a statistical technique': 1, 'statistical technique that': 1, 'technique that models': 1, 'that models and': 1, 'models and approximates': 1, 'and approximates the': 1, 'approximates the relationship': 1, 'the relationship between': 1, 'relationship between a': 1, 'between a dependent': 1, 'a dependent and': 1, 'dependent and one': 1, 'and one or': 1, 'one or more': 1, 'or more independent': 1, 'more independent variables.': 1, 'independent variables. This': 1, 'variables. This article': 1, 'This article will': 1, 'article will quickly': 1, 'will quickly introduce': 1, 'quickly introduce three': 1, 'introduce three commonly': 1, 'three commonly used': 1, 'commonly used regression': 1, 'used regression models': 1, 'regression models using': 1, 'models using R': 1, 'using R and': 1, 'R and the': 1, 'and the Boston': 1, 'the Boston housing': 1, 'Boston housing data-set:': 1, 'housing data-set: Ridge,': 1, 'data-set: Ridge, Lasso,': 1, 'Ridge, Lasso, and': 1, 'Lasso, and Elastic': 1, 'and Elastic Net.': 1, 'Elastic Net. First': 1, 'Net. First we': 1, 'First we need': 1, 'we need to': 3, 'need to understand': 1, 'to understand the': 1, 'understand the basics': 1, 'the basics of': 1, 'basics of regression': 1, 'of regression and': 1, 'regression and what': 1, 'and what parameters': 1, 'what parameters of': 1, 'parameters of the': 1, 'of the equation': 1, 'the equation are': 1, 'equation are changed': 1, 'are changed when': 1, 'changed when using': 1, 'when using a': 1, 'using a specific': 1, 'a specific model.': 1, 'specific model. Simple': 1, 'model. Simple linear': 1, 'Simple linear regression,': 1, 'linear regression, also': 1, 'regression, also known': 1, 'also known as': 1, 'known as ordinary': 1, 'as ordinary least': 1, 'ordinary least squares': 1, 'least squares (OLS)': 1, 'squares (OLS) attempts': 1, '(OLS) attempts to': 1, 'attempts to minimize': 1, 'to minimize the': 1, 'minimize the sum': 1, 'the sum of': 1, 'sum of error': 1, 'of error squared.': 1, 'error squared. The': 1, 'squared. The error': 1, 'The error in': 1, 'error in this': 1, 'in this case': 1, 'this case is': 1, 'case is the': 1, 'is the difference': 1, 'the difference between': 1, 'difference between the': 1, 'between the actual': 1, 'the actual data': 1, 'actual data point': 1, 'data point and': 1, 'point and its': 1, 'and its predicted': 1, 'its predicted value.': 1, 'predicted value. Visualization': 1, 'value. Visualization of': 1, 'Visualization of the': 1, 'of the squared': 1, 'the squared error': 1, 'squared error (from': 1, 'error (from Setosa.io)': 1, '(from Setosa.io) The': 1, 'Setosa.io) The equation': 1, 'The equation for': 1, 'equation for this': 1, 'for this model': 1, 'this model is': 1, 'model is referred': 1, 'is referred to': 1, 'referred to as': 1, 'to as the': 1, 'as the cost': 1, 'the cost function': 1, 'cost function and': 1, 'function and is': 1, 'and is a': 1, 'is a way': 1, 'a way to': 1, 'way to find': 1, 'to find the': 2, 'find the optimal': 2, 'the optimal error': 1, 'optimal error by': 1, 'error by minimizing': 1, 'by minimizing and': 1, 'minimizing and measuring': 1, 'and measuring it.': 1, 'measuring it. The': 1, 'it. The gradient': 1, 'The gradient descent': 1, 'gradient descent algorithm': 1, 'descent algorithm is': 1, 'algorithm is used': 1, 'is used to': 1, 'used to find': 1, 'the optimal cost': 1, 'optimal cost function': 1, 'cost function by': 1, 'function by going': 1, 'by going over': 1, 'going over a': 1, 'over a number': 2, 'a number of': 2, 'number of iterations.': 1, 'of iterations. But': 1, 'iterations. But the': 1, 'But the data': 1, 'the data we': 1, 'data we need': 1, 'need to define': 1, 'to define and': 1, 'define and analyze': 1, 'and analyze is': 1, 'analyze is not': 1, 'is not always': 1, 'not always so': 1, 'always so easy': 1, 'so easy to': 1, 'easy to characterize': 1, 'to characterize with': 1, 'characterize with the': 1, 'with the base': 1, 'the base OLS': 1, 'base OLS model.': 1, 'OLS model. Equation': 1, 'model. Equation for': 1, 'Equation for least': 1, 'for least ordinary': 1, 'least ordinary squares': 1, 'ordinary squares One': 1, 'squares One situation': 1, 'One situation is': 1, 'situation is the': 1, 'is the data': 1, 'the data showing': 1, 'data showing multi-collinearity,': 1, 'showing multi-collinearity, this': 1, 'multi-collinearity, this is': 1, 'this is when': 1, 'is when predictor': 1, 'when predictor variables': 1, 'predictor variables are': 1, 'variables are correlated': 1, 'are correlated to': 1, 'correlated to each': 1, 'to each other': 1, 'each other and': 1, 'other and to': 1, 'and to the': 1, 'to the response': 1, 'the response variable.': 1, 'response variable. To': 1, 'variable. To picture': 1, 'To picture this': 1, 'picture this let\\x92s': 1, 'this let\\x92s say': 1, 'let\\x92s say we\\x92re': 1, 'say we\\x92re doing': 1, 'we\\x92re doing a': 1, 'doing a study': 1, 'a study that': 1, 'study that looks': 1, 'that looks at': 1, 'looks at a': 1, 'at a response': 1, 'a response variable?\\x97?patient': 1, 'response variable?\\x97?patient weight,': 1, 'variable?\\x97?patient weight, and': 1, 'weight, and our': 1, 'and our predictor': 1, 'our predictor variables': 1, 'predictor variables would': 1, 'variables would be': 1, 'would be height,': 1, 'be height, sex,': 1, 'height, sex, and': 1, 'sex, and diet.': 1, 'and diet. The': 1, 'diet. The problem': 1, 'The problem here': 1, 'problem here is': 1, 'here is that': 1, 'is that height': 1, 'that height and': 1, 'height and sex': 1, 'and sex are': 1, 'sex are also': 1, 'are also correlated': 1, 'also correlated and': 1, 'correlated and can': 1, 'and can inflate': 1, 'can inflate the': 1, 'inflate the standard': 1, 'the standard error': 1, 'standard error of': 1, 'error of their': 1, 'of their coefficients': 1, 'their coefficients which': 1, 'coefficients which may': 1, 'which may make': 1, 'may make them': 1, 'make them seem': 1, 'them seem statistically': 1, 'seem statistically insignificant.': 1, 'statistically insignificant. To': 1, 'insignificant. To produce': 1, 'To produce a': 1, 'produce a more': 1, 'a more accurate': 1, 'more accurate model': 1, 'accurate model of': 1, 'model of complex': 1, 'of complex data': 1, 'complex data we': 1, 'data we can': 1, 'we can add': 1, 'can add a': 1, 'add a penalty': 1, 'a penalty term': 1, 'penalty term to': 2, 'term to the': 2, 'to the OLS': 2, 'the OLS equation.': 2, 'OLS equation. A': 1, 'equation. A penalty': 1, 'A penalty adds': 1, 'penalty adds a': 1, 'adds a bias': 1, 'a bias towards': 1, 'bias towards certain': 1, 'towards certain values.': 1, 'certain values. These': 1, 'values. These are': 1, 'These are known': 1, 'are known as': 1, 'known as L1': 1, 'as L1 regularization(Lasso': 1, 'L1 regularization(Lasso regression)': 1, 'regularization(Lasso regression) and': 1, 'regression) and L2': 1, 'and L2 regularization(ridge': 1, 'L2 regularization(ridge regression).The': 1, 'regularization(ridge regression).The best': 1, 'regression).The best model': 1, 'best model we': 1, 'model we can': 1, 'we can hope': 1, 'can hope to': 1, 'hope to come': 1, 'to come up': 1, 'come up with': 1, 'up with minimizes': 1, 'with minimizes both': 1, 'minimizes both the': 1, 'both the bias': 1, 'the bias and': 1, 'bias and the': 1, 'and the variance:': 1, 'the variance: Ridge': 1, 'variance: Ridge regression': 1, 'Ridge regression uses': 1, 'regression uses L2': 1, 'uses L2 regularization': 1, 'L2 regularization which': 1, 'regularization which adds': 1, 'which adds the': 1, 'adds the following': 1, 'the following penalty': 1, 'following penalty term': 1, 'OLS equation. L2': 1, 'equation. L2 regularization': 1, 'L2 regularization penalty': 1, 'regularization penalty term': 2, 'penalty term The': 1, 'term The L2': 1, 'The L2 term': 1, 'L2 term is': 1, 'term is equal': 1, 'is equal to': 2, 'equal to the': 2, 'to the square': 1, 'the square of': 1, 'square of the': 1, 'of the magnitude': 2, 'the magnitude of': 2, 'magnitude of the': 2, 'of the coefficients.': 1, 'the coefficients. In': 1, 'coefficients. In this': 1, 'In this case': 1, 'this case if': 1, 'case if lambda(?)': 1, 'if lambda(?) is': 1, 'lambda(?) is zero': 1, 'is zero then': 1, 'zero then the': 1, 'then the equation': 1, 'the equation is': 1, 'equation is the': 1, 'is the basic': 1, 'the basic OLS': 2, 'basic OLS but': 1, 'OLS but if': 1, 'but if it': 1, 'if it is': 1, 'it is greater': 1, 'is greater than': 1, 'greater than zero': 1, 'than zero then': 1, 'zero then we': 1, 'then we add': 1, 'we add a': 1, 'add a constraint': 1, 'a constraint to': 1, 'constraint to the': 1, 'to the coefficients.': 1, 'the coefficients. This': 1, 'coefficients. This constraint': 1, 'This constraint results': 1, 'constraint results in': 1, 'results in minimized': 1, 'in minimized coefficients': 1, 'minimized coefficients (aka': 1, 'coefficients (aka shrinkage)': 1, '(aka shrinkage) that': 1, 'shrinkage) that trend': 1, 'that trend towards': 1, 'trend towards zero': 1, 'towards zero the': 1, 'zero the larger': 1, 'the larger the': 1, 'larger the value': 2, 'the value of': 2, 'value of lambda.': 1, 'of lambda. Shrinking': 1, 'lambda. Shrinking the': 1, 'Shrinking the coefficients': 1, 'the coefficients leads': 1, 'coefficients leads to': 1, 'leads to a': 1, 'to a lower': 1, 'a lower variance': 1, 'lower variance and': 1, 'variance and in': 1, 'and in turn': 1, 'in turn a': 1, 'turn a lower': 1, 'a lower error': 1, 'lower error value.': 1, 'error value. Therefore': 1, 'value. Therefore Ridge': 1, 'Therefore Ridge regression': 1, 'Ridge regression decreases': 1, 'regression decreases the': 1, 'decreases the complexity': 1, 'the complexity of': 1, 'complexity of a': 1, 'of a model': 1, 'a model but': 1, 'model but does': 1, 'but does not': 1, 'does not reduce': 1, 'not reduce the': 1, 'reduce the number': 1, 'the number of': 1, 'number of variables,': 1, 'of variables, it': 1, 'variables, it rather': 1, 'it rather just': 1, 'rather just shrinks': 1, 'just shrinks their': 1, 'shrinks their effect.': 1, 'their effect. Lasso': 1, 'effect. Lasso regression': 1, 'Lasso regression Lasso': 1, 'regression Lasso regression': 1, 'Lasso regression uses': 1, 'regression uses the': 1, 'uses the L1': 1, 'the L1 penalty': 1, 'L1 penalty term': 1, 'penalty term and': 1, 'term and stands': 1, 'and stands for': 1, 'stands for Least': 1, 'for Least Absolute': 1, 'Least Absolute Shrinkage': 1, 'Absolute Shrinkage and': 1, 'Shrinkage and Selection': 1, 'and Selection Operator.': 1, 'Selection Operator. The': 1, 'Operator. The penalty': 1, 'The penalty applied': 1, 'penalty applied for': 1, 'applied for L2': 1, 'for L2 is': 1, 'L2 is equal': 1, 'to the absolute': 1, 'the absolute value': 1, 'absolute value of': 1, 'value of the': 1, 'of the coefficients:': 1, 'the coefficients: L1': 1, 'coefficients: L1 regularization': 1, 'L1 regularization penalty': 1, 'penalty term Similar': 1, 'term Similar to': 1, 'Similar to ridge': 1, 'to ridge regression,': 1, 'ridge regression, a': 1, 'regression, a lambda': 1, 'a lambda value': 2, 'lambda value of': 1, 'value of zero': 1, 'of zero spits': 1, 'zero spits out': 1, 'spits out the': 1, 'out the basic': 1, 'basic OLS equation,': 1, 'OLS equation, however': 1, 'equation, however given': 1, 'however given a': 1, 'given a suitable': 1, 'a suitable lambda': 1, 'suitable lambda value': 1, 'lambda value lasso': 1, 'value lasso regression': 1, 'lasso regression can': 1, 'regression can drive': 1, 'can drive some': 1, 'drive some coefficients': 1, 'some coefficients to': 1, 'coefficients to zero.': 1, 'to zero. The': 1, 'zero. The larger': 1, 'The larger the': 1, 'value of lambda': 1, 'of lambda the': 1, 'lambda the more': 1, 'the more features': 1, 'more features are': 1, 'features are shrunk': 1, 'are shrunk to': 1, 'shrunk to zero.': 1, 'to zero. This': 1, 'zero. This can': 1, 'This can eliminate': 1, 'can eliminate some': 1, 'eliminate some features': 1, 'some features entirely': 1, 'features entirely and': 1, 'entirely and give': 1, 'and give us': 1, 'give us a': 1, 'us a subset': 1, 'a subset of': 1, 'subset of predictors': 1, 'of predictors that': 1, 'predictors that helps': 1, 'that helps mitigate': 1, 'helps mitigate multi-collinearity': 1, 'mitigate multi-collinearity and': 1, 'multi-collinearity and model': 1, 'and model complexity.': 1, 'model complexity. Predictors': 1, 'complexity. Predictors not': 1, 'Predictors not shrunk': 1, 'not shrunk towards': 1, 'shrunk towards zero': 1, 'towards zero signify': 1, 'zero signify that': 1, 'signify that they': 1, 'that they are': 1, 'they are important': 1, 'are important and': 1, 'important and thus': 1, 'and thus L1': 1, 'thus L1 regularization': 1, 'L1 regularization allows': 1, 'regularization allows for': 1, 'allows for feature': 1, 'for feature selection': 1, 'feature selection (sparse': 1, 'selection (sparse selection).': 1, '(sparse selection). A': 1, 'selection). A third': 1, 'A third commonly': 1, 'third commonly used': 1, 'commonly used model': 1, 'used model of': 1, 'model of regression': 1, 'of regression is': 1, 'regression is the': 1, 'is the Elastic': 1, 'the Elastic Net': 1, 'Elastic Net which': 1, 'Net which incorporates': 1, 'which incorporates penalties': 1, 'incorporates penalties from': 1, 'penalties from both': 1, 'from both L1': 1, 'both L1 and': 1, 'L1 and L2': 1, 'and L2 regularization:': 1, 'L2 regularization: In': 1, 'regularization: In addition': 1, 'In addition to': 1, 'addition to setting': 1, 'to setting and': 1, 'setting and choosing': 1, 'and choosing a': 1, 'choosing a lambda': 1, 'lambda value elastic': 1, 'value elastic net': 1, 'elastic net also': 1, 'net also allows': 1, 'also allows us': 1, 'allows us to': 1, 'us to tune': 2, 'to tune the': 1, 'tune the alpha': 1, 'the alpha parameter': 1, 'alpha parameter where': 1, 'parameter where ??': 1, 'where ?? =': 1, '?? = 0': 1, '= 0 corresponds': 1, '0 corresponds to': 1, 'corresponds to ridge': 1, 'to ridge and': 1, 'ridge and ??': 1, 'and ?? =': 1, '?? = 1': 2, '= 1 to': 1, '1 to lasso.': 1, 'to lasso. Simply': 1, 'lasso. Simply put,': 1, 'Simply put, if': 1, 'put, if you': 1, 'if you plug': 1, 'you plug in': 1, 'plug in 0': 1, 'in 0 for': 1, '0 for alpha,': 1, 'for alpha, the': 1, 'alpha, the penalty': 1, 'the penalty function': 1, 'penalty function reduces': 1, 'function reduces to': 1, 'reduces to the': 1, 'to the L1': 1, 'the L1 (ridge)': 1, 'L1 (ridge) term': 1, '(ridge) term and': 1, 'term and if': 1, 'and if we': 1, 'if we set': 1, 'we set alpha': 1, 'set alpha to': 1, 'alpha to 1': 1, 'to 1 we': 1, '1 we get': 1, 'we get the': 1, 'get the L2': 1, 'the L2 (lasso)': 1, 'L2 (lasso) term.': 1, '(lasso) term. Therefore': 1, 'term. Therefore we': 1, 'Therefore we can': 1, 'we can choose': 1, 'can choose an': 1, 'choose an alpha': 1, 'an alpha value': 1, 'alpha value between': 1, 'value between 0': 1, 'between 0 and': 1, '0 and 1': 1, 'and 1 to': 1, '1 to optimize': 1, 'to optimize the': 1, 'optimize the elastic': 1, 'the elastic net.': 1, 'elastic net. Effectively': 1, 'net. Effectively this': 1, 'Effectively this will': 1, 'this will shrink': 1, 'will shrink some': 1, 'shrink some coefficients': 1, 'some coefficients and': 1, 'coefficients and set': 1, 'and set some': 1, 'set some to': 1, 'some to 0': 1, 'to 0 for': 1, '0 for sparse': 1, 'for sparse selection.': 1, 'sparse selection. As': 1, 'selection. As we': 1, 'As we mentioned': 1, 'we mentioned in': 1, 'mentioned in the': 1, 'in the previous': 1, 'the previous sections,': 1, 'previous sections, lambda': 1, 'sections, lambda values': 1, 'lambda values have': 1, 'values have a': 1, 'have a large': 1, 'a large effect': 1, 'large effect on': 1, 'effect on coefficients': 1, 'on coefficients so': 1, 'coefficients so now': 1, 'so now we': 1, 'now we will': 1, 'we will compute': 1, 'will compute and': 1, 'compute and chose': 1, 'and chose a': 1, 'chose a suitable': 1, 'a suitable one.': 1, 'suitable one. Here': 1, 'one. Here we': 1, 'Here we perform': 1, 'we perform a': 1, 'perform a cross': 1, 'a cross validation': 1, 'cross validation and': 1, 'validation and take': 1, 'and take a': 1, 'take a peek': 1, 'a peek at': 1, 'peek at the': 1, 'at the lambda': 1, 'the lambda value': 1, 'lambda value corresponding': 1, 'value corresponding to': 1, 'corresponding to the': 1, 'to the lowest': 1, 'the lowest prediction': 1, 'lowest prediction error': 1, 'prediction error before': 1, 'error before fitting': 1, 'before fitting the': 1, 'fitting the data': 1, 'the data to': 1, 'data to the': 1, 'to the model': 1, 'the model and': 1, 'model and viewing': 1, 'and viewing the': 1, 'viewing the coefficients.': 1, 'the coefficients. We': 1, 'coefficients. We can': 1, 'We can see': 2, 'can see here': 1, 'see here that': 1, 'here that certain': 1, 'that certain coefficients': 1, 'certain coefficients have': 1, 'coefficients have been': 1, 'have been pushed': 1, 'been pushed towards': 1, 'pushed towards zero': 1, 'towards zero and': 1, 'zero and minimized': 1, 'and minimized while': 1, 'minimized while RM': 1, 'while RM (number': 1, 'RM (number of': 1, '(number of rooms)': 1, 'of rooms) has': 1, 'rooms) has a': 1, 'has a significantly': 1, 'a significantly higher': 1, 'significantly higher weight': 1, 'higher weight than': 1, 'weight than the': 1, 'than the rest': 1, 'the rest Performing': 1, 'rest Performing Lasso': 1, 'Performing Lasso regression': 1, 'Lasso regression The': 1, 'regression The steps': 1, 'The steps will': 1, 'steps will be': 1, 'will be identical': 1, 'be identical to': 1, 'identical to what': 1, 'to what we': 1, 'what we have': 1, 'we have done': 1, 'have done for': 1, 'done for ridge': 1, 'for ridge regression.': 1, 'ridge regression. The': 1, 'regression. The value': 1, 'The value of': 1, 'value of alpha': 1, 'of alpha is': 1, 'alpha is the': 1, 'is the only': 1, 'the only change': 1, 'only change here': 1, 'change here (remember': 1, 'here (remember ??': 1, '(remember ?? =': 1, '= 1 denotes': 1, '1 denotes lasso)': 1, 'denotes lasso) Performing': 1, 'lasso) Performing Elastic': 1, 'Performing Elastic Net': 2, 'Elastic Net regression': 1, 'Net regression Performing': 1, 'regression Performing Elastic': 1, 'Elastic Net requires': 1, 'Net requires us': 1, 'requires us to': 1, 'to tune parameters': 1, 'tune parameters to': 1, 'parameters to identify': 1, 'to identify the': 1, 'identify the best': 1, 'the best alpha': 1, 'best alpha and': 1, 'alpha and lambda': 2, 'and lambda values': 1, 'lambda values and': 1, 'values and for': 1, 'and for this': 1, 'for this we': 1, 'this we need': 1, 'need to use': 1, 'to use the': 1, 'use the caret': 1, 'the caret package.': 1, 'caret package. We': 1, 'package. We will': 1, 'We will tune': 1, 'will tune the': 1, 'tune the model': 1, 'the model by': 1, 'model by iterating': 1, 'by iterating over': 1, 'iterating over a': 1, 'number of alpha': 1, 'of alpha and': 1, 'and lambda pairs': 1, 'lambda pairs and': 1, 'pairs and we': 1, 'and we can': 1, 'we can see': 1, 'can see which': 1, 'see which pair': 1, 'which pair has': 1, 'pair has the': 1, 'has the lowest': 1, 'the lowest associated': 1, 'lowest associated error.': 1, 'associated error. We': 1, 'error. We can': 1, 'can see that': 1, 'see that the': 1, 'that the R': 1, 'the R mean-squared': 1, 'R mean-squared values': 1, 'mean-squared values using': 1, 'values using all': 1, 'using all three': 1, 'all three models': 1, 'three models were': 1, 'models were very': 1, 'were very close': 1, 'very close to': 1, 'close to each': 1, 'to each other,': 1, 'each other, but': 1, 'other, but both': 1, 'but both did': 1, 'both did marginally': 1, 'did marginally perform': 1, 'marginally perform better': 1, 'perform better than': 1, 'better than ridge': 1, 'than ridge regression': 1, 'ridge regression (Lasso': 1, 'regression (Lasso having': 1, '(Lasso having done': 1, 'having done best).': 1, 'done best). Lasso': 1, 'best). Lasso regression': 1, 'Lasso regression also': 1, 'regression also showed': 1, 'also showed the': 1, 'showed the highest': 1, 'the highest RÂ²': 1, 'highest RÂ² value.': 1} \n","\n","----Task(d):Dictionary with 10 highest values:\n","we need to : 3\n","to find the : 2\n","find the optimal : 2\n","over a number : 2\n","a number of : 2\n","penalty term to : 2\n","term to the : 2\n","to the OLS : 2\n","the OLS equation. : 2\n","regularization penalty term : 2\n","\n","\n","Task(e,f) Combined\n","----All the sentences with the most repeated tri-grams----\n","\n","First we need to understand the basics of regression and what parameters of the equation are changed when using a specific model.\n","But the data we need to define and analyze is not always so easy to characterize with the base OLS model.\n","= 1 denotes lasso)\n","Performing Elastic Net regression\n","Performing Elastic Net requires us to tune parameters to identify the best alpha and lambda values and for this we need to use the caret package.\n","\n","\n","Task(g,h) Combined\n","----Concatenated Result----\n","\n","First we need to understand the basics of regression and what parameters of the equation are changed when using a specific model.But the data we need to define and analyze is not always so easy to characterize with the base OLS model.= 1 denotes lasso)\n","Performing Elastic Net regression\n","Performing Elastic Net requires us to tune parameters to identify the best alpha and lambda values and for this we need to use the caret package.\n"],"name":"stdout"}]}]}